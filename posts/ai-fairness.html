<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Are Algorithms Biased? - Karan Praharaj">
    <title>Are Algorithms Biased? — Karan Praharaj</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500&family=Inter:wght@400;500&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
</head>

<body>
    <header class="header">
        <a href="/" class="logo">Karan Praharaj</a>
        <nav class="nav">
            <a href="/#about">About</a>
            <a href="/writing.html">Writing</a>
        </nav>
    </header>

    <div class="divider"></div>

    <main class="main main-top">
        <article class="post">
            <header class="post-header">
                <h1>Are Algorithms Biased?</h1>
                <p class="post-subtitle">Algorithms reinforce human biases and stereotypes. This is dangerous.</p>
                <p class="post-date">June 27, 2020</p>
            </header>

            <div class="post-content">
                <p>After the end of the Second World War, the Nuremberg trials laid bare the atrocities conducted in
                    medical research by the Nazis. In the aftermath of the trials, the medical sciences established a
                    set of rules — The Nuremberg Code — to control future experiments involving human subjects. The
                    Nuremberg Code has influenced medical codes of ethics around the world, as has the exposure of
                    experiments that had failed to follow it even three decades later, such as the infamous <a
                        href="https://en.wikipedia.org/wiki/Tuskegee_syphilis_experiment">Tuskegee syphilis
                        experiment</a>.</p>

                <p>The direct negative impact of AI experiments and applications on users isn't quite as inhumane as
                    that of the Tuskegee and Nazi experimentations, but in the face of an overwhelming and growing body
                    of evidence of algorithms being biased against certain demographic cohorts, it is important that a
                    dialogue takes place sooner or later. AI systems can be biased based on who builds them, the way
                    they are developed, and how they're eventually deployed. This is known as algorithmic bias.</p>

                <p>While the data sciences have not developed a Nuremberg Code of their own yet, the social implications
                    of research in artificial intelligence are starting to be addressed in some curricula. But even as
                    the debates are starting to sprout up, what is still lacking is a discipline-wide discussion to
                    grapple with questions of how to tackle societal and historical inequities that are reinforced by AI
                    algorithms.</p>

                <p>We are flawed creatures. Every single decision we make involves a certain kind of bias. However,
                    algorithms haven't proven to be much better. Ideally, we would want our algorithms to make
                    better-informed decisions devoid of biases so as to ensure better social justice, i.e., equal
                    opportunities for individuals and groups (such as minorities) within society to access resources,
                    have their voices heard, and be represented in society.</p>

                <p>When these algorithms do the job of amplifying racial, social and gender inequality, instead of
                    alleviating it; it becomes necessary to take stock of the ethical ramifications and potential
                    malevolence of the technology.</p>

                <p>This essay was motivated by two flashpoints: the racial inequality discussion that is now raging on
                    worldwide, and Yann LeCun's altercation with Timnit Gebru on Twitter which was caused due to a
                    disagreement over a downsampled image of Barack Obama (left) that was depixelated to a picture of a
                    white man (right) by a face upsampling machine learning (ML) model.</p>

                <p>The (rather explosive) argument was sparked by a tweet by LeCun where he says that the resulting face
                    was that of a white man due to a bias in data that trained the model. In simple terms, LeCun said
                    that the results could be improved by increasing the number of black faces that the model sees.
                    Gebru responded sharply that the harms of ML systems cannot be solely reduced to biased data.</p>

                <p>In most baseline ML algorithms, the model fits better to the attributes or patterns that occur most
                    frequently across various data points. For example, if you were to design an AI recruiting tool to
                    review the résumés of applicants for a software engineering position, you would first need to train
                    it with a dataset of past candidates which contains details like "experience", "qualifications",
                    "degree(s) held", "past projects" etc. For every datapoint, the algorithm of the hiring tool would
                    need a decision or a "label", so as to "learn" how to make a decision for a given applicant by
                    observing patterns in their résumé.</p>

                <p>For an industry where the gender disparity in representation is large, it is reasonable to assume
                    that a large majority of the data points will be male applicants. And this collective imbalance in
                    the data ends up being interpreted by the algorithm as a useful pattern in the data rather than
                    undesirable noise which is to be ignored. Consequently, it teaches itself that male candidates are
                    more preferable than female candidates.</p>

                <p>I wish that this was merely an imaginary, exaggerated example that I used to prove my point. <a
                        href="https://in.reuters.com/article/amazon-com-jobs-automation/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idINKCN1MK0AH">It
                        is not.</a></p>

                <p>LeCun wasn't wrong in his assessment because in the case of that specific model, training the model
                    on a dataset that contains faces of black people (as opposed to one that contains mainly white
                    faces) would not have given rise to an output as absurd as that.</p>

                <p>The misunderstanding clearly seems to emanate from the interpretation of the word "bias" — which in
                    any discussion about the social impact of ML/AI seems to get crushed under the burden of its own
                    weight. As Sebastian Raschka puts it, "the term <strong>bias</strong> in ML is heavily overloaded".
                    It has multiple senses that can all be mistaken for each other:</p>

                <ol>
                    <li><strong>bias</strong> (as in mathematical bias unit)</li>
                    <li>Societal <strong>bias</strong></li>
                    <li>Inductive <strong>bias</strong> (which is dependent on decisions taken to build the model)</li>
                    <li><strong>bias</strong>-variance decomposition of a loss function</li>
                    <li>Dataset <strong>bias</strong></li>
                </ol>

                <p>I imagine that a lot of gaps in communication could be covered by just being a little more precise
                    when we use these terms. But the one upside to a public feud between a Turing Award winner and a
                    pioneer of algorithmic fairness is that people in the community are bound to talk about it. This
                    will hopefully mean an increased awareness among researchers about the social implications of their
                    findings and with that, hopefully, an increased sense of responsibility to mitigate the harms.</p>

                <p>Learning algorithms have inductive biases going beyond the biases in data too, sure. But if the data
                    has a little bias, it is amplified by these systems, thereby causing high biases to be learnt by the
                    model. Simply put, creating a 100% non-biased dataset is practically impossible. Any dataset picked
                    by humans is cherry-picked and non-exhaustive. Our social cognitive biases result in inadvertent
                    cherry-picking of data. This biased data, when fed to a data-variant model (a model whose decisions
                    are heavily influenced by the data it sees) encodes these societal, racial, gender, cultural and
                    political biases and bakes them into the ML model.</p>

                <p>These problems are exacerbated, once they are applied to products. A couple of years ago, Jacky
                    Alciné <a href="https://twitter.com/jackyalcine/status/615329515909156865">pointed out</a> that the
                    image recognition algorithms in <a
                        href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/">Google
                        Photos were classifying his black friends as "gorillas."</a> Google apologised for the blunder
                    and assured to resolve the issue. However, instead of coming up with a proper solution, it simply
                    blocked the algorithm from identifying gorillas at all.</p>

                <p>It might seem surprising that a company of Google's size was unable to come up with a solution to
                    this. But this only goes to show that training an algorithm that is consistent and fair isn't an
                    easy proposition, not least when it is not trained and tested on a diverse set of categories that
                    represent various demographic cohorts of the population proportionately.</p>

                <p>Problems of algorithmic bias are not limited to image/video tasks and they manifest themselves in
                    language tasks too.</p>

                <p><a href="https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf">Language is always "situated"</a>, i.e.,
                    it depends on external references for its understanding and the receiver(s) must be in a position to
                    resolve these references. This therefore means that the text used to train models carries latent
                    information about the author and the situation, albeit to varying degrees.</p>

                <p>Due to the situatedness of language, any language data set inevitably carries with it a demographic
                    bias. For example, some speech to text transcription models tend to have higher error rates for
                    African Americans, Arabs and South Asians as compared to Americans and Europeans. This is because
                    the corpus that the speech recognition models are trained are dominated by utterances of people from
                    western countries. This causes the system to be good at interpreting European and American accents
                    but subpar at transcribing speech from other parts of the world.</p>

                <p>Another example in this space is the gender biases in existing word embeddings (which are learned
                    through a neural networks) that show females having a higher association with "less-cerebral"
                    occupations while males tend to be associated with purportedly "more-cerebral" or higher paying
                    occupations.</p>

                <figure>
                    <img src="../img/tablefair.png" alt="Gender bias scores in Universal Sentence Encoder">
                    <figcaption>Gender bias scores associated with various occupations in the Universal Sentence Encoder
                        embedding model. Positive scores are female-biased, negative scores are male-biased.
                    </figcaption>
                </figure>

                <p>For ML Researchers it would be easy to punt the blame and absolve themselves of all responsibility,
                    but it is imperative for them to acknowledge that they—knowingly or otherwise—build the base layer
                    of AI products for a lot of companies that are devoid of AI expertise. These companies, without the
                    knowledge of fine-tuning and tweaking models, use pre-trained models, as they are, put out on the
                    internet by ML researchers (for e.g. - models like GloVe, BERT, ResNet, YOLO etc).</p>

                <p>Deploying these models without explicitly recalibrating them to account for demographic differences
                    can thus lead to issues of exclusion and overgeneralisation of people along the way. The buck stops
                    with the researchers who must own up responsibility for the other side of the coin.</p>

                <p>It is also easy to blame the data and not the algorithm. Pinning the blame on just the data is
                    irresponsible and akin to saying that the racist child isn't racist because he was taught the racism
                    by his racist father.</p>

                <p>More than we need to improve the data, it is the algorithms that need to be made more robust, less
                    sensitive and less prone to being biased by data that is skewed. This needs to be a responsibility
                    for anyone who does research. In the meantime, de-bias the data.</p>

                <p>The guiding question for deployment of algorithms in the real world should always be <em>"would a
                        false answer be worse than no answer?"</em></p>

                <hr>

                <p><em>Thanks to Nayan K, <a href="https://twitter.com/naga_karthik7">Naga Karthik</a> and Bina Praharaj
                        for reviewing drafts of this.</em></p>

                <h2>References</h2>
                <ol class="references">
                    <li>Buolamwini, J., Gebru, T. Gender Shades: Intersectional accuracy disparities in commercial
                        gender classification. in <em>Conference on Fairness, Accountability and Transparency,
                            2018</em>.</li>
                    <li><a
                            href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html">Facial
                            Recognition Is Accurate, if You're a White Guy</a> by Steve Lohr</li>
                    <li>Krishnapriya, KS., et al. Characterizing the Variability in Face Recognition Accuracy Relative
                        to Race in <em>CVPR Workshops, 2019</em>.</li>
                    <li><a href="https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf">Life of Language</a> by Martin Kay,
                        Stanford University</li>
                    <li><a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html">Text
                            Embedding Models Contain Bias. Here's Why That Matters.</a> by Ben Packer et al., Google AI
                    </li>
                    <li>Bolukbasi, T., et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
                        Embeddings in <em>NeurIPS 2016</em>.</li>
                </ol>
            </div>
        </article>
    </main>

    <footer class="footer">
        <p class="copyright">&copy; 2026 Karan Praharaj</p>
        <nav class="footer-nav">
            <a href="mailto:hello@karanpraharaj.com">Email</a>
            <a href="https://twitter.com/karanpraharaj" target="_blank" rel="noopener">Twitter</a>
            <a href="https://github.com/karanpraharaj" target="_blank" rel="noopener">GitHub</a>
        </nav>
    </footer>
</body>

</html>